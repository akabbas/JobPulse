#!/usr/bin/env python3
"""
Test script for the FetchHire Advanced Scraper
"""

import asyncio
import json
import time
from scrapers.advanced_scraper import AdvancedJobScraper

async def test_advanced_scraper():
    """Test the advanced scraper functionality"""
    print("ğŸš€ Testing FetchHire Advanced Scraper")
    print("=" * 50)
    
    # Initialize the advanced scraper
    scraper = AdvancedJobScraper(cache_dir="cache", max_cache_age_hours=24)
    
    print("âœ… Advanced Scraper Features:")
    print("   â€¢ Selenium/Headless Browser for LinkedIn")
    print("   â€¢ Async Scraping for multiple sources")
    print("   â€¢ Intelligent Caching (24-hour cache)")
    print("   â€¢ Proxy Rotation (when configured)")
    print("   â€¢ Rate Limiting and Anti-Detection")
    print("   â€¢ Duplicate Removal")
    print()
    
    # Test individual components
    print("ğŸ” Testing Individual Components:")
    
    # Test LinkedIn scraping with Selenium
    print("   ğŸ“Š Testing LinkedIn scraping with Selenium...")
    start_time = time.time()
    linkedin_jobs = await scraper.scrape_linkedin_advanced(max_pages=1)
    linkedin_time = time.time() - start_time
    print(f"   âœ… LinkedIn: {len(linkedin_jobs)} jobs in {linkedin_time:.2f}s")
    
    # Test Remote OK scraping
    print("   ğŸŒ Testing Remote OK scraping...")
    start_time = time.time()
    remote_jobs = await scraper.scrape_remote_ok_advanced()
    remote_time = time.time() - start_time
    print(f"   âœ… Remote OK: {len(remote_jobs)} jobs in {remote_time:.2f}s")
    
    # Test full async scraping
    print("   âš¡ Testing full async scraping...")
    start_time = time.time()
    all_jobs = await scraper.scrape_all_sources_advanced()
    total_time = time.time() - start_time
    print(f"   âœ… Total: {len(all_jobs)} unique jobs in {total_time:.2f}s")
    
    print()
    print("ğŸ“ˆ Performance Analysis:")
    print(f"   â€¢ LinkedIn scraping: {linkedin_time:.2f}s")
    print(f"   â€¢ Remote OK scraping: {remote_time:.2f}s")
    print(f"   â€¢ Total async time: {total_time:.2f}s")
    print(f"   â€¢ Speedup vs sequential: {((linkedin_time + remote_time) / total_time):.1f}x")
    
    # Test caching
    print()
    print("ğŸ’¾ Testing Caching:")
    print("   ğŸ”„ Running same scrape again (should use cache)...")
    start_time = time.time()
    cached_jobs = await scraper.scrape_all_sources_advanced()
    cache_time = time.time() - start_time
    print(f"   âœ… Cached scrape: {len(cached_jobs)} jobs in {cache_time:.2f}s")
    print(f"   ğŸ“‰ Cache speedup: {(total_time / cache_time):.1f}x faster")
    
    # Test skills analytics
    print()
    print("ğŸ¯ Testing Skills Analytics:")
    analytics = scraper.get_skills_analytics(all_jobs)
    print(f"   ğŸ“Š Total jobs analyzed: {analytics['total_jobs']}")
    print(f"   ğŸ·ï¸  Total unique skills: {analytics['total_skills']}")
    print(f"   ğŸ“ Sources: {', '.join(analytics['sources'])}")
    
    # Show top skills
    print("   ğŸ† Top 10 Skills:")
    for i, (skill, count) in enumerate(list(analytics['top_skills'].items())[:10], 1):
        print(f"      {i:2d}. {skill}: {count}")
    
    # Save results
    print()
    print("ğŸ’¾ Saving Results:")
    scraper.save_jobs_to_file(all_jobs, 'test_advanced_results.json')
    print("   âœ… Results saved to 'test_advanced_results.json'")
    
    # Show sample jobs
    print()
    print("ğŸ“‹ Sample Jobs:")
    for i, job in enumerate(all_jobs[:3], 1):
        print(f"   {i}. {job['title']} at {job['company']}")
        print(f"      ğŸ“ {job['location']} | ğŸ’° {job.get('salary', 'N/A')}")
        print(f"      ğŸ·ï¸  Skills: {', '.join(job['tags'][:5])}")
        print()
    
    print("ğŸ‰ Advanced scraper test completed successfully!")
    return all_jobs

def test_proxy_configuration():
    """Test proxy configuration"""
    print("ğŸŒ Testing Proxy Configuration:")
    scraper = AdvancedJobScraper()
    
    # Test proxy loading
    proxies = scraper._load_proxies()
    if proxies:
        print(f"   âœ… Loaded {len(proxies)} proxies")
        for i, proxy in enumerate(proxies[:3], 1):
            print(f"      {i}. {proxy}")
    else:
        print("   âš ï¸  No proxies configured")
        print("   ğŸ’¡ Add proxies to 'proxies.txt' for better success rates")
    
    print()

def test_selenium_setup():
    """Test Selenium setup"""
    print("ğŸ¤– Testing Selenium Setup:")
    scraper = AdvancedJobScraper()
    
    try:
        scraper._init_selenium_driver()
        if scraper.driver:
            print("   âœ… Selenium WebDriver initialized successfully")
            print("   ğŸ­ Headless browser ready for LinkedIn scraping")
        else:
            print("   âŒ Selenium WebDriver failed to initialize")
            print("   ğŸ’¡ Make sure Chrome/Chromium is installed")
    except Exception as e:
        print(f"   âŒ Selenium error: {e}")
        print("   ğŸ’¡ Install Chrome/Chromium and chromedriver")
    
    # Cleanup
    scraper._cleanup_selenium_driver()
    print()

if __name__ == "__main__":
    print("ğŸ§ª FetchHire Advanced Scraper Test Suite")
    print("=" * 50)
    
    # Test configuration
    test_proxy_configuration()
    test_selenium_setup()
    
    # Run async test
    try:
        jobs = asyncio.run(test_advanced_scraper())
        print(f"\nâœ… Test completed! Scraped {len(jobs)} jobs with advanced features.")
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        print("ğŸ’¡ Make sure all dependencies are installed:")
        print("   pip install -r requirements_advanced.txt") 
    print("âœ… All tests completed successfully!")
    print("ğŸš€ Your advanced scraper is ready to use!") 